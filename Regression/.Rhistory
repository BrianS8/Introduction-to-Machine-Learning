library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # a java-based platform
library(pdp)          # model visualization
library(ggplot2)      # model visualization
library(lime)         # model visualization
library(rsample)      # data splitting
library(gbm)          # basic implementation
library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # a java-based platform
library(pdp)          # model visualization
library(ggplot2)      # model visualization
library(lime)         # model visualization
install.packages("lime")
install.packages("furrr")
library(rsample)      # data splitting
library(gbm)          # basic implementation
library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # a java-based platform
library(pdp)          # model visualization
library(ggplot2)      # model visualization
library(lime)         # model visualization
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
predicciones
###############################
#### Tecnicas de regresion ####
###############################
# En esta sección veremos un conjunto de técnicas para arboles de decision, random forest y boosting
# Librerías
library(tree) # Funciones para arboles de regresion y clasificacion
library(rpart) # Funciones para arboles de regresion y clasificacion
library(rpart.plot) # Graficas relacionadas a rpart
library(C50) # Algoritmos para arboles de clasificacion
library(MASS)
library(ggplot2)
library(ggpubr)
library(randomForest)
library(tidyverse)
library(dplyr)
### Ejemplo arbol de decisión
data("Boston")
head(Boston)
train <- sample(1:nrow(Boston), size = nrow(Boston)/2)
arbol_regresion <- tree(formula = medv ~ ., data = Boston, subset = train,
split = "deviance")
plot(x = arbol_regresion, type = "proportional")
text(x = arbol_regresion, splits = TRUE, pretty = 0,
cex = 0.8, col = "firebrick")
## Por defecto tenemos:
# mincut = 5: número mínimo de observaciones que debe de tener al menos uno de los nodos hijos para que se produzca la división.
# minsize = 10: número mínimo de observaciones que debe de tener un nodo para que pueda dividirse.
# deepth = 31: profundidad máxima que puede alcanzar el árbol.
# Podado
set.seed(3)
cv_arbol <- cv.tree(arbol_regresion, K = 10)
cv_arbol
resultados_cv <- data.frame(n_nodos = cv_arbol$size,
deviance = cv_arbol$dev,
alpha = cv_arbol$k)
p1 <- ggplot(data = resultados_cv, aes(x = n_nodos, y = deviance)) +
geom_line() +
geom_point() +
labs(title = "Error vs tamaño del árbol") + theme_bw()
p2 <- ggplot(data = resultados_cv, aes(x = alpha, y = deviance)) +
geom_line() +
geom_point() +
labs(title = "Error vs hiperparámetro alpha") + theme_bw()
ggarrange(p1, p2)
arbol_pruning <- prune.tree(tree = arbol_regresion, best = 8)
plot(x = arbol_pruning, type = "proportional")
text(x = arbol_pruning, splits = TRUE, pretty = 0,
cex = 0.8, col = "firebrick")
predicciones <- predict(arbol_pruning, newdata = Boston[-train,])
test_mse     <- mean((predicciones - Boston[-train, "medv"])^2)
paste("Error de test (mse) del árbol de regresión tras podado:", (round(test_mse,2)))
# Librerías
library(tree) # Funciones para arboles de regresion y clasificacion
library(rpart) # Funciones para arboles de regresion y clasificacion
library(rpart.plot) # Graficas relacionadas a rpart
library(C50) # Algoritmos para arboles de clasificacion
library(MASS)
library(ggplot2)
library(ggpubr)
library(randomForest)
library(tidyverse)
library(dplyr)
data(Boston)
# Estimar el precio de la vivienda nueva, dado el conjunto de datos disponibles
data("Boston")
ncol(Boston)
set.seed(1)
train <- sample(1:nrow(Boston), size = nrow(Boston)/2)
modelo_bagging <- randomForest(medv ~ ., data = Boston,
subset = train, mtry = 13)
modelo_bagging
predicciones <- predict(object = modelo_bagging, newdata = Boston[-train,])
test_mse     <- mean((predicciones - Boston[-train, "medv"])^2)
paste("Error de test (mse) del modelo obtenido por bagging es:",
(round(test_mse,2)))
# Importancia
modelo_bagging <- randomForest(medv ~ ., data = Boston,
mtry = 13, ntree = 500,
importance = TRUE)
importancia_pred <- as.data.frame(importance(modelo_bagging,
scale = TRUE))
importancia_pred <- rownames_to_column(importancia_pred,
var = "variable")
p1 <- ggplot(data = importancia_pred,
aes(x = reorder(variable, `%IncMSE`),
y = `%IncMSE`,fill = `%IncMSE`)) +
labs(x = "variable", title = "Reducción de MSE") +
geom_col() +
coord_flip() +
theme_bw() +
theme(legend.position = "bottom")
p2 <- ggplot(data = importancia_pred,
aes(x = reorder(variable, IncNodePurity),
y = IncNodePurity,
fill = IncNodePurity)) +
labs(x = "variable", title = "Reducción de pureza") +
geom_col() +
coord_flip() +
theme_bw() +
theme(legend.position = "bottom")
ggarrange(p1, p2)
help("tail")
tuning_rf_nodesize <- function(df, y, size = NULL, ntree = 500){
# Esta función devuelve el out-of-bag-MSE de un modelo randomForest en función del tamaño mínimo de los nodos terminales (nodesize).
# Argumentos:
#   df = data frame con los predictores y variable respuesta
#   y  = nombre de la variable respuesta
#   sizes = tamaños evaluados
#   ntree = número de árboles creados en el modelo randomForest
if (is.null(size)){
size <- seq(from = 1, to = nrow(df), by = 5)
}
oob_mse <- rep(NA, length(size))
for (i in seq_along(size)) {
set.seed(123)
f <- formula(paste(y,"~ ."))
modelo_rf <- randomForest(formula = f, data = df, mtry = 5,
ntree = ntree, nodesize = i)
oob_mse[i] <- tail(modelo_rf$mse, n = 1)
}
results <- data_frame(size, oob_mse)
return(results)
}
hiperparametro_nodesize <-  tuning_rf_nodesize(df = Boston, y = "medv",
size = c(1:20))
hiperparametro_nodesize
help(randomForest)
hiperparametro_nodesize %>% arrange(oob_mse)
gplot(data = hiperparametro_nodesize, aes(x = size, y = oob_mse)) +
scale_x_continuous(breaks = hiperparametro_nodesize$size) +
geom_line() +
geom_point() +
geom_point(data = hiperparametro_nodesize %>% arrange(oob_mse) %>% head(1),
color = "red") +
labs(title = "Evolución del out-of-bag-error vs nodesize",
x = "nº observaciones en nodos terminales") +
theme_bw()
modelo_randomforest <- randomForest(medv ~ ., data = Boston,
subset = train, mtry = 5 ,
ntree = 500, nodesize = 5,
importance = TRUE)
ggplot(data = hiperparametro_nodesize, aes(x = size, y = oob_mse)) +
scale_x_continuous(breaks = hiperparametro_nodesize$size) +
geom_line() +
geom_point() +
geom_point(data = hiperparametro_nodesize %>% arrange(oob_mse) %>% head(1),
color = "red") +
labs(title = "Evolución del out-of-bag-error vs nodesize",
x = "nº observaciones en nodos terminales") +
theme_bw()
train
modelo_randomforest <- randomForest(medv ~ ., data = Boston,
subset = train, mtry = 5 ,
ntree = 500, nodesize = 5,
importance = TRUE)
oob_mse <- data.frame(oob_mse = modelo_randomforest$mse,
arboles = seq_along(modelo_randomforest$mse))
ggplot(data = oob_mse, aes(x = arboles, y = oob_mse )) +
geom_line() +
labs(title = "Evolución del out-of-bag-error vs número árboles",
x = "nº árboles") +
theme_bw()
# Ajuste final
predicciones <- predict(object = modelo_randomforest,
newdata = Boston[-train, ])
test_mse <- mean((predicciones - Boston[-train, "medv"])^2)
paste("Error de test (mse) del modelo:", round(test_mse, 2))
importancia_pred <- as.data.frame(importance(modelo_randomforest, scale = TRUE))
importancia_pred <- rownames_to_column(importancia_pred, var = "variable")
p1 <- ggplot(data = importancia_pred, aes(x = reorder(variable, `%IncMSE`),
y = `%IncMSE`,
fill = `%IncMSE`)) +
labs(x = "variable", title = "Reducción de MSE") +
geom_col() +
coord_flip() +
theme_bw() +
theme(legend.position = "bottom")
p2 <- ggplot(data = importancia_pred, aes(x = reorder(variable, IncNodePurity),
y = IncNodePurity,
fill = IncNodePurity)) +
labs(x = "variable", title = "Reducción de pureza") +
geom_col() +
coord_flip() +
theme_bw() +
theme(legend.position = "bottom")
ggarrange(p1, p2)
###############################
#### Tecnicas de regresion ####
###############################
rm(list = ls())
# En esta sección veremos un conjunto de técnicas para arboles de decision, random forest y boosting
# Librerías
library(tree) # Funciones para arboles de regresion y clasificacion
library(rpart) # Funciones para arboles de regresion y clasificacion
library(rpart.plot) # Graficas relacionadas a rpart
library(C50) # Algoritmos para arboles de clasificacion
library(MASS)
library(ggplot2)
library(ggpubr)
library(randomForest)
library(tidyverse)
library(dplyr)
library(gbm)
library(caret)
data(Boston)
set.seed(1)
train <- sample(1:nrow(Boston), size = nrow(Boston)/2)
cv_error  <- vector("numeric")
n_arboles <- vector("numeric")
shrinkage <- vector("numeric")
cv_error  <- vector("numeric")
n_arboles <- vector("numeric")
shrinkage <- vector("numeric")
for (i in c(0.001, 0.01, 0.1)) {
set.seed(123)
arbol_boosting <- gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian",
n.trees = 20000,
interaction.depth = 1,
shrinkage = i,
n.minobsinnode = 10,
bag.fraction = 0.5,
cv.folds = 5)
cv_error  <- c(cv_error, arbol_boosting$cv.error)
n_arboles <- c(n_arboles, seq_along(arbol_boosting$cv.error))
shrinkage <- c(shrinkage, rep(i, length(arbol_boosting$cv.error)))
}
error <- data.frame(cv_error, n_arboles, shrinkage)
ggplot(data = error, aes(x = n_arboles, y = cv_error,
color = as.factor(shrinkage))) +
geom_smooth() +
labs(title = "Evolución del cv-error", color = "shrinkage") +
theme_bw() +
theme(legend.position = "bottom")
## Complejidad de los arboles (Número de divisiones)
cv_error  <- vector("numeric")
n_arboles <- vector("numeric")
interaction.depth <- vector("numeric")
for (i in c(1, 3, 5, 10)) {
set.seed(123)
arbol_boosting <- gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian",
n.trees = 5000,
interaction.depth = i,
shrinkage = 0.01,
n.minobsinnode = 10,
bag.fraction = 0.5,
cv.folds = 5)
cv_error  <- c(cv_error, arbol_boosting$cv.error)
n_arboles <- c(n_arboles, seq_along(arbol_boosting$cv.error))
interaction.depth <- c(interaction.depth,
rep(i, length(arbol_boosting$cv.error)))
}
error <- data.frame(cv_error, n_arboles, interaction.depth)
ggplot(data = error, aes(x = n_arboles, y = cv_error,
color = as.factor(interaction.depth))) +
geom_smooth() +
labs(title = "Evolución del cv-error", color = "interaction.depth") +
theme_bw() +
theme(legend.position = "bottom")
## Mi
source('~/Documents/Introduction Machine Learning/Course/Introduction-to-Machine-Learning/Regression/Clase_Boosting.R', echo=TRUE)
error
error
set.seed(123)
validacion <- trainControl(## 10-fold CV
method = "cv",
number = 10)
tuning_grid <-  expand.grid(interaction.depth = c(1, 5, 9),
n.trees = c(100, 1000, 2000, 3000),
shrinkage = c(0.1, 0.01, 0.001),
n.minobsinnode = c(1, 10, 20))
set.seed(123)
mejor_modelo <- train(medv ~ ., data = Boston[train, ],
method = "gbm",
trControl = validacion,
verbose = FALSE,
tuneGrid = tuning_grid)
# Se muestran los hiperparámetros del mejor modelo
mejor_modelo$bestTune
names(Boston)
df <- Boston[,c("medv","zn","indus","chas")]
train <- sample(1:nrow(Boston), size = nrow(Boston)/2)
df <- Boston[,c("medv","zn","indus","chas")]
train <- sample(1:nrow(Boston), size = nrow(Boston)/2)
modelo <- lm(medv ~ ., data = train )
train <- sample(1:nrow(Boston), size = nrow(Boston)/2)
modelo <- lm(medv ~ ., data = Boston )
modelo <- lm(medv ~ ., data = Boston[train,] )
summary(modelo)
predicciones <- predict(object = modelo, newdata = Auto[-train, ])
df <- Boston[,c("medv","zn","indus","chas")]
train <- sample(1:nrow(df), size = nrow(df)/2)
modelo <- lm(medv ~ ., data = df[train,] )
predicciones <- predict(object = modelo, newdata = df[-train, ])
test_mse     <- mean((predicciones - df[-train, "medv"])^2)
test_mse
### Ejemplo arbol de decisión
data("Boston")
head(Boston)
df <- Boston[,c("medv","zn","indus","chas")]
arbol_regresion <- tree(formula = medv ~ ., data = df, subset = train,
split = "deviance")
plot(x = arbol_regresion, type = "proportional")
text(x = arbol_regresion, splits = TRUE, pretty = 0,
cex = 0.8, col = "firebrick")
## Por defecto tenemos:
# mincut = 5: número mínimo de observaciones que debe de tener al menos uno de los nodos hijos para que se produzca la división.
# minsize = 10: número mínimo de observaciones que debe de tener un nodo para que pueda dividirse.
# deepth = 31: profundidad máxima que puede alcanzar el árbol.
# Podado
set.seed(3)
cv_arbol <- cv.tree(arbol_regresion, K = 10)
cv_arbol
resultados_cv <- data.frame(n_nodos = cv_arbol$size,
deviance = cv_arbol$dev,
alpha = cv_arbol$k)
arbol_regresion <- tree(formula = medv ~ ., data = df, subset = train,
split = "deviance")
arbol_regresion
set.seed(3)
cv_arbol <- cv.tree(arbol_regresion, K = 10)
, random forest y boosting
# Librerías
library(tree) # Funciones para arboles de regresion y clasificacion
library(rpart) # Funciones para arboles de regresion y clasificacion
library(rpart.plot) # Graficas relacionadas a rpart
library(C50) # Algoritmos para arboles de clasificacion
library(MASS)
library(ggplot2)
library(ggpubr)
library(randomForest)
library(tidyverse)
library(dplyr)
### Ejemplo arbol de decisión
data("Boston")
head(Boston)
df <- Boston[,c("medv","zn","indus","chas")]
arbol_regresion <- tree(formula = medv ~ ., data = Boston, subset = train,
split = "deviance")
plot(x = arbol_regresion, type = "proportional")
text(x = arbol_regresion, splits = TRUE, pretty = 0,
cex = 0.8, col = "firebrick")
## Por defecto tenemos:
# mincut = 5: número mínimo de observaciones que debe de tener al menos uno de los nodos hijos para que se produzca la división.
# minsize = 10: número mínimo de observaciones que debe de tener un nodo para que pueda dividirse.
# deepth = 31: profundidad máxima que puede alcanzar el árbol.
# Podado
set.seed(3)
cv_arbol <- cv.tree(arbol_regresion, K = 10)
cv_arbol
data("Boston")
head(Boston)
df <- Boston[,c("medv","zn","indus","chas")]
arbol_regresion <- tree(formula = medv ~ ., data = df, subset = train,
split = "deviance")
plot(x = arbol_regresion, type = "proportional")
text(x = arbol_regresion, splits = TRUE, pretty = 0,
cex = 0.8, col = "firebrick")
set.seed(3)
cv_arbol <- cv.tree(arbol_regresion, K = 10)
cv_arbol
resultados_cv <- data.frame(n_nodos = cv_arbol$size,
deviance = cv_arbol$dev,
alpha = cv_arbol$k)
p1 <- ggplot(data = resultados_cv, aes(x = n_nodos, y = deviance)) +
geom_line() +
geom_point() +
labs(title = "Error vs tamaño del árbol") + theme_bw()
p2 <- ggplot(data = resultados_cv, aes(x = alpha, y = deviance)) +
geom_line() +
geom_point() +
labs(title = "Error vs hiperparámetro alpha") + theme_bw()
ggarrange(p1, p2)
arbol_pruning <- prune.tree(tree = arbol_regresion, best = 8)
plot(x = arbol_pruning, type = "proportional")
text(x = arbol_pruning, splits = TRUE, pretty = 0,
cex = 0.8, col = "firebrick")
predicciones <- predict(arbol_pruning, newdata = Boston[-train,])
test_mse     <- mean((predicciones - Boston[-train, "medv"])^2)
test_mse
rm(list=ls())
# Librerías
library(tree) # Funciones para arboles de regresion y clasificacion
library(rpart) # Funciones para arboles de regresion y clasificacion
library(rpart.plot) # Graficas relacionadas a rpart
library(C50) # Algoritmos para arboles de clasificacion
library(MASS)
library(ggplot2)
library(ggpubr)
library(randomForest)
library(tidyverse)
library(dplyr)
library(dplyr)
library(psych)
library(GGally)
library(ggplot2)
library(gridExtra)
library(lmtest)
library(corrplot)
library(car)
library(ISLR)
require(knitr)
library(AER)
library(stargazer)
library(scales)
library(MASS)
library(mvtnorm)
df <- Boston[,c("medv","zn","indus","chas")]
train <- sample(1:nrow(df), size = nrow(df)/2)
modelo <- lm(medv ~ ., data = df[train,] )
predicciones <- predict(object = modelo, newdata = df[-train, ])
test_mse     <- mean((predicciones - df[-train, "medv"])^2)
test_mse
# En esta sección veremos un conjunto de técnicas para arboles de decision, random forest y boosting
# Librerías
library(tree) # Funciones para arboles de regresion y clasificacion
library(rpart) # Funciones para arboles de regresion y clasificacion
library(rpart.plot) # Graficas relacionadas a rpart
library(C50) # Algoritmos para arboles de clasificacion
library(MASS)
library(ggplot2)
library(ggpubr)
library(randomForest)
library(tidyverse)
library(dplyr)
### Ejemplo arbol de decisión
data("Boston")
head(Boston)
df <- Boston[,c("medv","zn","indus","chas")]
train <- sample(1:nrow(df), size = nrow(df)/2)
arbol_regresion <- tree(formula = medv ~ ., data = df, subset = train,
split = "deviance")
plot(x = arbol_regresion, type = "proportional")
text(x = arbol_regresion, splits = TRUE, pretty = 0,
cex = 0.8, col = "firebrick")
set.seed(3)
cv_arbol <- cv.tree(arbol_regresion, K = 10)
cv_arbol
resultados_cv <- data.frame(n_nodos = cv_arbol$size,
deviance = cv_arbol$dev,
alpha = cv_arbol$k)
p1 <- ggplot(data = resultados_cv, aes(x = n_nodos, y = deviance)) +
geom_line() +
geom_point() +
labs(title = "Error vs tamaño del árbol") + theme_bw()
p2 <- ggplot(data = resultados_cv, aes(x = alpha, y = deviance)) +
geom_line() +
geom_point() +
labs(title = "Error vs hiperparámetro alpha") + theme_bw()
###############################
#### Tecnicas de regresion ####
###############################
# En esta sección veremos un conjunto de técnicas para arboles de decision, random forest y boosting
# Librerías
library(tree) # Funciones para arboles de regresion y clasificacion
library(rpart) # Funciones para arboles de regresion y clasificacion
library(rpart.plot) # Graficas relacionadas a rpart
library(C50) # Algoritmos para arboles de clasificacion
library(MASS)
library(ggplot2)
library(ggpubr)
library(randomForest)
library(tidyverse)
library(dplyr)
### Ejemplo arbol de decisión
data("Boston")
head(Boston)
df <- Boston[,c("medv","zn","indus","chas")]
train <- sample(1:nrow(df), size = nrow(df)/2)
arbol_regresion <- tree(formula = medv ~ ., data = df, subset = train,
split = "deviance")
plot(x = arbol_regresion, type = "proportional")
text(x = arbol_regresion, splits = TRUE, pretty = 0,
cex = 0.8, col = "firebrick")
## Por defecto tenemos:
# mincut = 5: número mínimo de observaciones que debe de tener al menos uno de los nodos hijos para que se produzca la división.
# minsize = 10: número mínimo de observaciones que debe de tener un nodo para que pueda dividirse.
# deepth = 31: profundidad máxima que puede alcanzar el árbol.
# Podado
set.seed(3)
cv_arbol <- cv.tree(arbol_regresion, K = 10)
cv_arbol
