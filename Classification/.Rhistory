rm(list=ls())
library(AmesHousing)
library(rsample)
AmesHousing = make_ames()
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
rm(list = ls())
library(dplyr)
library(psych)
library(GGally)
library(ggplot2)
library(gridExtra)
library(lmtest)
library(corrplot)
library(car)
library(ISLR)
require(knitr)
library(AER)
library(stargazer)
library(scales)
library(MASS)
library(mvtnorm)
library(rsample)
options(scipen=999)
datos <-  force(state.x77)
colnames(datos) <- c("habitantes","ingresos","analfabetismo", "esp_vida","asesinatos","universitarios", "heladas", "area")
datos <- data.frame(datos)
datos$densidad_pobl <- datos$habitantes * 1000 / datos$area
data <- initial_split(datos, prop = 0.7)
train <-  training(data)
500*0.8
500/0.8
50*0.8
50*0.7
a <- round(cor(x = dtrain, method = "pearson"), 3)
multi.hist(x = dtrain, dcol = c("blue", "red"), dlty = c("dotted", "solid"),main = "")
ggpairs(dtrain, lower = list(continuous = "smooth"),
diag = list(continuous = "bar"), axisLabels = "none")
pairs(dtrain)
rm(list = ls())
library(dplyr)
library(psych)
library(GGally)
library(ggplot2)
library(gridExtra)
library(lmtest)
library(corrplot)
library(car)
library(ISLR)
require(knitr)
library(AER)
library(stargazer)
library(scales)
library(MASS)
library(mvtnorm)
library(rsample)
options(scipen=999)
data(state)
datos <-  force(state.x77)
colnames(datos) <- c("habitantes","ingresos","analfabetismo", "esp_vida","asesinatos","universitarios", "heladas", "area")
datos <- data.frame(datos)
datos$densidad_pobl <- datos$habitantes * 1000 / datos$area
# Datos de entrenamiento y de validación
data   <- initial_split(datos, prop = 0.7)
dtrain <-  training(data)
dtest  <- testing(data)
# Analisis grafico de las variables
a <- round(cor(x = dtrain, method = "pearson"), 3)
multi.hist(x = dtrain, dcol = c("blue", "red"), dlty = c("dotted", "solid"),main = "")
ggpairs(dtrain, lower = list(continuous = "smooth"),
diag = list(continuous = "bar"), axisLabels = "none")
pairs(dtrain)
modelo <- lm(esp_vida ~ habitantes + ingresos + analfabetismo + asesinatos + universitarios + heladas + area + densidad_pobl, data = datos )
summary(modelo)
step(object = modelo, direction = "both", trace = 1)
modelo <- (lm(formula = esp_vida ~ habitantes + asesinatos + universitarios + heladas, data = datos))
summary(modelo)
plot1 <- ggplot(data = datos, aes(habitantes, modelo$residuals)) +
geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
theme_bw()
plot2 <- ggplot(data = datos, aes(asesinatos, modelo$residuals)) +
geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
theme_bw()
plot3 <- ggplot(data = datos, aes(universitarios, modelo$residuals)) +
geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
theme_bw()
plot4 <- ggplot(data = datos, aes(heladas, modelo$residuals)) +
geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
theme_bw()
grid.arrange(plot1, plot2, plot3, plot4)
plot(modelo)
plot1 <- ggplot(data = datos, aes(habitantes, modelo$residuals)) +
geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
theme_bw()
plot2 <- ggplot(data = datos, aes(asesinatos, modelo$residuals)) +
geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
theme_bw()
plot3 <- ggplot(data = datos, aes(universitarios, modelo$residuals)) +
geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
theme_bw()
plot4 <- ggplot(data = datos, aes(heladas, modelo$residuals)) +
geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
theme_bw()
grid.arrange(plot1, plot2, plot3, plot4)
plot(modelo)
plot(modelo)
plot(modelo)
qqnorm(modelo$residuals)
qqline(modelo$residuals)
shapiro.test(modelo$residuals)
#Homocedasticidad
ggplot(data = datos, aes(modelo$fitted.values, modelo$residuals)) +
geom_point() +
geom_smooth(color = "firebrick", se = FALSE) +
geom_hline(yintercept = 0) +
theme_bw()
bptest(modelo)
corrplot(cor(dplyr::select(datos, habitantes, asesinatos,universitarios,heladas)), method = "number", tl.col = "black")
vif(modelo)
# Autocorrelación
dwt(modelo, alternative = "two.sided")
influencePlot(modelo)
rm(list = ls())
library(dplyr)
library(psych)
library(GGally)
library(ggplot2)
library(gridExtra)
library(lmtest)
library(corrplot)
library(car)
library(ISLR)
require(knitr)
library(AER)
library(stargazer)
library(scales)
library(MASS)
library(mvtnorm)
library(rsample)
regressor = svm(formula = esp_vida ~ habitantes + asesinatos +
universitarios + heladas,
data = data,
type = 'eps-regression',
kernel = 'radial')
# Predicting a new result
y_pred = predict(regressor, data.frame(Level = 6.5))
library(e1071)
regressor = svm(formula = esp_vida ~ habitantes + asesinatos +
universitarios + heladas,
data = data,
type = 'eps-regression',
kernel = 'radial')
# Predicting a new result
y_pred = predict(regressor, data.frame(Level = 6.5))
regressor = svm(formula = esp_vida ~ habitantes + asesinatos +
universitarios + heladas,
data = dtrain,
type = 'eps-regression',
kernel = 'radial')
data   <- initial_split(datos, prop = 0.7)
dtrain <- training(data)
dtest  <- testing(data)
rm(list = ls())
library(dplyr)
library(psych)
library(GGally)
library(ggplot2)
library(gridExtra)
library(lmtest)
library(corrplot)
library(car)
library(ISLR)
require(knitr)
library(AER)
library(stargazer)
library(scales)
library(MASS)
library(mvtnorm)
library(rsample)
options(scipen=999)
data(state)
datos <-  force(state.x77)
colnames(datos) <- c("habitantes","ingresos","analfabetismo", "esp_vida","asesinatos","universitarios", "heladas", "area")
datos <- data.frame(datos)
datos$densidad_pobl <- datos$habitantes * 1000 / datos$area
# Datos de entrenamiento y de validación
data   <- initial_split(datos, prop = 0.7)
dtrain <- training(data)
dtest  <- testing(data)
# Analisis grafico de las variables
a <- round(cor(x = dtrain, method = "pearson"), 3)
multi.hist(x = dtrain, dcol = c("blue", "red"), dlty = c("dotted", "solid"),main = "")
ggpairs(dtrain, lower = list(continuous = "smooth"),
diag = list(continuous = "bar"), axisLabels = "none")
pairs(dtrain)
modelo <- lm(esp_vida ~ habitantes + ingresos + analfabetismo + asesinatos + universitarios + heladas + area + densidad_pobl, data = datos )
summary(modelo)
# Selección del mejor modelo dado el criterio AIC
step(object = modelo, direction = "both", trace = 1)
modelo <- (lm(formula = esp_vida ~ habitantes + asesinatos + universitarios + heladas, data = datos))
summary(modelo)
plot1 <- ggplot(data = datos, aes(habitantes, modelo$residuals)) +
geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
theme_bw()
plot2 <- ggplot(data = datos, aes(asesinatos, modelo$residuals)) +
geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
theme_bw()
plot3 <- ggplot(data = datos, aes(universitarios, modelo$residuals)) +
geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
theme_bw()
plot4 <- ggplot(data = datos, aes(heladas, modelo$residuals)) +
geom_point() + geom_smooth(color = "firebrick") + geom_hline(yintercept = 0) +
theme_bw()
grid.arrange(plot1, plot2, plot3, plot4)
plot(modelo)
shapiro.test(modelo$residuals)
#Homocedasticidad
ggplot(data = datos, aes(modelo$fitted.values, modelo$residuals)) +
geom_point() +
geom_smooth(color = "firebrick", se = FALSE) +
geom_hline(yintercept = 0) +
theme_bw()
# Breusch-Pagan test
bptest(modelo)
# No multicolinealidad
corrplot(cor(dplyr::select(datos, habitantes, asesinatos,universitarios,heladas)), method = "number", tl.col = "black")
# Analisis de inflacion de la varianza
vif(modelo)
# Autocorrelación
dwt(modelo, alternative = "two.sided")
# Grafico de influencias
influencePlot(modelo)
#### SVM
library(e1071)
regressor = svm(formula = esp_vida ~ habitantes + asesinatos +
universitarios + heladas,
data = dtrain,
type = 'eps-regression',
kernel = 'radial')
# Predicting a new result
y_pred = predict(regressor, data.frame(Level = 6.5))
View(dtrain)
regressor = svm(formula = esp_vida ~ habitantes + asesinatos +
universitarios + heladas,
data = dtrain,
type = 'eps-regression',
kernel = 'radial')
# Predicting a new result
y_pred = predict(regressor, data.frame(Level = 6.5))
regressor
predicciones <- predict(object = regressor, newdata = dtest)
data(state)
datos <-  force(state.x77)
View(datos)
# Analisis grafico de las variables
a <- round(cor(x = dtrain, method = "pearson"), 3)
multi.hist(x = dtrain, dcol = c("blue", "red"), dlty = c("dotted", "solid"),main = "")
ggpairs(dtrain, lower = list(continuous = "smooth"),
diag = list(continuous = "bar"), axisLabels = "none")
modelo <- lm(esp_vida ~ habitantes + ingresos + analfabetismo + asesinatos + universitarios + heladas + area + densidad_pobl, data = datos )
summary(modelo)
modelo <- lm(esp_vida ~ habitantes + ingresos + analfabetismo + asesinatos + universitarios + heladas + area + densidad_pobl, data = datos )
list = ls())
library(dplyr)
library(psych)
library(GGally)
library(ggplot2)
library(gridExtra)
library(lmtest)
library(corrplot)
library(car)
library(ISLR)
require(knitr)
library(AER)
library(stargazer)
library(scales)
library(MASS)
library(mvtnorm)
library(rsample)
library(e1071)
options(scipen=999)
data(state)
datos <-  force(state.x77)
colnames(datos) <- c("habitantes","ingresos","analfabetismo", "esp_vida","asesinatos","universitarios", "heladas", "area")
datos <- data.frame(datos)
datos$densidad_pobl <- datos$habitantes * 1000 / datos$area
# Datos de entrenamiento y de validación
data   <- initial_split(datos, prop = 0.7)
dtrain <- training(data)
dtest  <- testing(data)
modelo <- lm(esp_vida ~ habitantes + ingresos + analfabetismo + asesinatos + universitarios + heladas + area + densidad_pobl, data = datos )
modelo <- lm(esp_vida ~ habitantes + ingresos + analfabetismo + asesinatos + universitarios + heladas + area + densidad_pobl, data = dtrain )
summary(modelo)
# Selección del mejor modelo dado el criterio AIC
step(object = modelo, direction = "both", trace = 1)
modelo <- lm(formula = esp_vida ~ asesinatos + universitarios + heladas, data = dtrain)
summary(modelo)
plot(modelo)
# Normalidad
shapiro.test(modelo$residuals)
#Homocedasticidad
ggplot(data = datos, aes(modelo$fitted.values, modelo$residuals)) +
geom_point() +
geom_smooth(color = "firebrick", se = FALSE) +
geom_hline(yintercept = 0) +
theme_bw()
bptest(modelo)
corrplot(cor(dplyr::select(datos, habitantes, asesinatos,universitarios,heladas)), method = "number", tl.col = "black")
vif(modelo)
# Autocorrelación
dwt(modelo, alternative = "two.sided")
# Grafico de influencias
influencePlot(modelo)
predicciones <- predict(object = modelo, newdata = dtest)
test_mse     <- mean((predicciones - Boston[-train, "medv"])^2)
paste("Error de test (mse) del modelo obtenido por bagging es:",
(round(test_mse,2)))
predicciones <- predict(object = modelo, newdata = dtest)
predicciones
names(dtest)
test_mse     <- mean((predicciones - dtest[,"esp_vida" ])^2)
paste("Error de test (mse) del modelo obtenido por bagging es:",
(round(test_mse,2)))
head(dtrain)
modelo_svm = svm(formula = esp_vida ~ habitantes + asesinatos +
universitarios + heladas,
data = dtrain,
type = 'eps-regression')
predicciones <- predict(object = modelo_svm, newdata = dtest)
test_mse     <- mean((predicciones - dtest[,"esp_vida" ])^2)
paste("Error de test (mse) del modelo obtenido por bagging es:",
(round(test_mse,2)))
arbol_regresion <- tree(formula = esp_vida ~ habitantes + asesinatos +
universitarios + heladas, data = dtrain,
split = "deviance")
# Librerías
library(tree) # Funciones para arboles de regresion y clasificacion
library(rpart) # Funciones para arboles de regresion y clasificacion
library(rpart.plot) # Graficas relacionadas a rpart
library(C50) # Algoritmos para arboles de clasificacion
library(MASS)
library(ggplot2)
library(ggpubr)
library(randomForest)
library(tidyverse)
library(dplyr)
arbol_regresion <- tree(formula = esp_vida ~ habitantes + asesinatos +
universitarios + heladas, data = dtrain,
split = "deviance")
plot(x = arbol_regresion, type = "proportional")
text(x = arbol_regresion, splits = TRUE, pretty = 0,
cex = 0.8, col = "firebrick")
set.seed(3)
cv_arbol <- cv.tree(arbol_regresion, K = 10)
cv_arbol
resultados_cv <- data.frame(n_nodos = cv_arbol$size,
deviance = cv_arbol$dev,
alpha = cv_arbol$k)
resultados_cv
p1 <- ggplot(data = resultados_cv, aes(x = n_nodos, y = deviance)) +
geom_line() +
geom_point() +-
labs(title = "Error vs tamaño del árbol") + theme_bw()
p2 <- ggplot(data = resultados_cv, aes(x = alpha, y = deviance)) +
geom_line() +
geom_point() +
labs(title = "Error vs hiperparámetro alpha") + theme_bw()
ggarrange(p1, p2)
p1 <- ggplot(data = resultados_cv, aes(x = n_nodos, y = deviance)) +
geom_line() +
geom_point() +-
labs(title = "Error vs tamaño del árbol") + theme_bw()
p1 <- ggplot(data = resultados_cv, aes(x = n_nodos, y = deviance)) +
geom_line() +
geom_point() +
labs(title = "Error vs tamaño del árbol") + theme_bw()
p2 <- ggplot(data = resultados_cv, aes(x = alpha, y = deviance)) +
geom_line() +
geom_point() +
labs(title = "Error vs hiperparámetro alpha") + theme_bw()
ggarrange(p1, p2)
arbol_pruning <- prune.tree(tree = arbol_regresion, best = 5)
plot(x = arbol_pruning, type = "proportional")
text(x = arbol_pruning, splits = TRUE, pretty = 0,
cex = 0.8, col = "firebrick")
arbol_pruning <- prune.tree(tree = arbol_regresion, best = 5)
plot(x = arbol_pruning, type = "proportional")
arbol_pruning <- prune.tree(tree = arbol_regresion, best = 8)
arbol_pruning <- prune.tree(tree = arbol_regresion, best = 1)
plot(x = arbol_pruning, type = "proportional")
arbol_pruning <- prune.tree(tree = arbol_regresion, best = 2)
plot(x = arbol_pruning, type = "proportional")
text(x = arbol_pruning, splits = TRUE, pretty = 0,
cex = 0.8, col = "firebrick")
predicciones <- predict(arbol_pruning, newdata = dtrain)
test_mse     <- mean((predicciones - dtest[,"esp_vida" ])^2)
paste("Error de test (mse) del árbol de regresión tras podado:", (round(test_mse,2)))
arbol_pruning <- prune.tree(tree = arbol_regresion, best = 5)
# plot(x = arbol_pruning, type = "proportional")
# text(x = arbol_pruning, splits = TRUE, pretty = 0,
#      cex = 0.8, col = "firebrick")
predicciones <- predict(arbol_pruning, newdata = dtrain)
test_mse     <- mean((predicciones - dtest[,"esp_vida" ])^2)
paste("Error de test (mse) del árbol de regresión tras podado:", (round(test_mse,2)))
library(tree) # Funciones para arboles de regresion y clasificacion
library(rpart) # Funciones para arboles de regresion y clasificacion
library(rpart.plot) # Graficas relacionadas a rpart
library(C50) # Algoritmos para arboles de clasificacion
library(MASS)
library(ggplot2)
library(ggpubr)
library(randomForest)
library(tidyverse)
library(dplyr)
tuning_rf_nodesize <- function(df, y, size = NULL, ntree = 500){
# Esta función devuelve el out-of-bag-MSE de un modelo randomForest en función del tamaño mínimo de los nodos terminales (nodesize).
# Argumentos:
#   df = data frame con los predictores y variable respuesta
#   y  = nombre de la variable respuesta
#   sizes = tamaños evaluados
#   ntree = número de árboles creados en el modelo randomForest
if (is.null(size)){
size <- seq(from = 1, to = nrow(df), by = 5)
}
oob_mse <- rep(NA, length(size))
for (i in seq_along(size)) {
set.seed(123)
f <- formula(paste(y,"~ ."))
modelo_rf <- randomForest(formula = f, data = df, mtry = 5,
ntree = ntree, nodesize = i)
oob_mse[i] <- tail(modelo_rf$mse, n = 1)
}
results <- data_frame(size, oob_mse)
return(results)
}
names(dtrain)
hiperparametro_nodesize <-  tuning_rf_nodesize(df = dtrain, y = "esp_vida",
size = c(1:20))
hiperparametro_nodesize
hiperparametro_nodesize %>% arrange(oob_mse)
ggplot(data = hiperparametro_nodesize, aes(x = size, y = oob_mse)) +
scale_x_continuous(breaks = hiperparametro_nodesize$size) +
geom_line() +
geom_point() +
geom_point(data = hiperparametro_nodesize %>% arrange(oob_mse) %>% head(1),
color = "red") +
labs(title = "Evolución del out-of-bag-error vs nodesize",
x = "nº observaciones en nodos terminales") +
theme_bw()
hiperparametro_nodesize$size
ggplot(data = hiperparametro_nodesize, aes(x = size, y = oob_mse)) +
scale_x_continuous(breaks = hiperparametro_nodesize$size) +
geom_line() +
geom_point() +
geom_point(data = hiperparametro_nodesize %>% arrange(oob_mse) %>% head(1),
color = "red") +
labs(title = "Evolución del out-of-bag-error vs nodesize",
x = "nº observaciones en nodos terminales") +
theme_bw()
ggplot(data = hiperparametro_nodesize, aes(x = size, y = oob_mse)) +
scale_x_continuous(breaks = hiperparametro_nodesize$size) +
geom_line() +
geom_point() +
geom_point(data = hiperparametro_nodesize %>% arrange(oob_mse) %>% head(1),) +
labs(title = "Evolución del out-of-bag-error vs nodesize",
x = "nº observaciones en nodos terminales") +
theme_bw()
hiperparametro_nodesize %>% arrange(oob_mse) %>% head(1)
modelo_randomforest <- randomForest(esp_vida ~ habitantes + asesinatos +
universitarios + heladas, data = dtrain,
data = dtrain,
subset = train, mtry = 5 ,
ntree = 500, nodesize = 3,
importance = TRUE)
modelo_randomforest <- randomForest(esp_vida ~ habitantes + asesinatos +
universitarios + heladas, data = dtrain,
subset = train, mtry = 5,
ntree = 500, nodesize = 3,
importance = TRUE)
library(tidyverse)
library(ISLR)
datos <- Default
datos <- datos %>%
select(default, balance) %>%
mutate(default = recode(default,
"No"  = 0,
"Yes" = 1))
setwd()
getwd()
setwd("/Users/bsmith/Documents/Introduction Machine Learning/Course/Introductio-to-Machine-Learning")
setwd("/Users/bsmith/Documents/Introduction Machine Learning/Course/Introduction-to-Machine-Learning")
write_csv(datos, 'datos.csv')
setwd("/Users/bsmith/Documents/Introduction Machine Learning/Course/Introduction-to-Machine-Learning/Classification")
library(openintro)
data(email)
str(email)
install.packages("openintro")
library(openintro)
data(email)
str(email)
write_csv(email, 'email_spam.csv')
View(email)
email("intercept") <- rep(1,1:3921)
rep(1,1:2)
help(rep)
rep(1,2)
email("intercept") <- rep(1,3921)
email["intercept"] <- rep(1,3921)
View(email)
write_csv(email, 'email_spam.csv')
