{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> Clasificador Naïve Bayes </center> </h1> \n",
    "\n",
    "Este algoritmo simple esta basado en el Teorema de Bayes. Este asume que el efecto de una característica particular en una clase es independiente de otras características\n",
    "$$ P(h|D) = \\frac{P(D|h)P(D)}{P(h)}$$\n",
    "\n",
    "Supone independencia entre los predictores, conocido como interdependencia condicional de clase, esto lo hace ingenuo\n",
    "\n",
    "$P(h):$ probabilidad de que la hipótesis h sea cierta (independientemente de los datos). Esto se conoce como la probabilidad previa de h.\n",
    "<br/>\n",
    "$P(D):$ probabilidad de los datos (independientemente de la hipótesis). Esto se conoce como probabilidad previa.\n",
    "\n",
    "$P(D|h):$ probabilidad de los datos D, dado h.\n",
    "<br/>\n",
    "$P(h|D):$ probabilidad de que la hipótesis h dado un conjunto de datos D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo\n",
    "\n",
    "El algoritmo de Naïve Bayes funciona de la siguiente manera:\n",
    "1. Calcular las probabilidades a priori para las clases dadas\n",
    "2. Determinar la probabildad con cada atributo para cada clase\n",
    "3. Reemplazar los valores obtenidos en la probabilidad de Bayes\n",
    "4. Identificar a que clase pertenece la probabilidad obtenida\n",
    "\n",
    "### Ventajas\n",
    "1. Util para predicción multiclase/multinomial\n",
    "2. Se necesita menos datos de entrenamiento\n",
    "3. Funciona con entradas categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_spam = pd.read_csv('email_spam.csv')\n",
    "email_spam['winner'] = email_spam['winner'].replace({'yes':1, 'no':0})\n",
    "y = np.array(email_spam[\"spam\"])\n",
    "X = np.array(email_spam[[\"intercept\",\"to_multiple\",\"format\",\"cc\",\"attach\",\"dollar\", \"winner\",\"inherit\",\n",
    "               \"password\",\"re_subj\",\"exclaim_subj\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)\n",
    "NaiveBayes = GaussianNB()\n",
    "NB_model = NaiveBayes.fit(X_train,y_train)\n",
    "y_pred = NB_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1\n",
    "Evalue si el modelo Naïve Bayes es adecuado para distinguir entre correos spam y no-spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> K Vecinos más cercanos </center> </h1> \n",
    "\n",
    "<p>Este es un algoritmo simple con el que se pueden abordar problemas de reconocimiento de patrones, minería de datos/texto, detección de intrusos, entre otros, en el que el conjunto de datos puede ser no paramétrico (No hay suposición explicita sobre la forma funcional de los datos) y este algoritmos esta basado en instancias, es decir, el algoritmo no aprende directamente del modelo, sino de las instancias de información, que son utilizadas como \"conocimiento\" para poder clasificar los datos nuveos.\n",
    "\n",
    "El algoritmo funciona de la siguiente manera:\n",
    "1. Se selecciona el punto en el que se encuentra el registro al cual se le va a realizar la predicción\n",
    "2. Dado un hiperparametro ```k```, que se entiende el número de vecinos por cada grupo,  se seleccionan los vecinos más cercanos a este punto usando las siguientes medidas:\n",
    "    1. Distancia Euclidiana\n",
    "    2. Distancia Manhattan\n",
    "    3. Distancia Minkowski\n",
    "3. Por medio de \"votación\", se selecciona la clase de este nuevo elemento\n",
    "</p>\n",
    "\n",
    "### K\n",
    "<p>No existe un número para todo tipo de datos, por lo que depende del criterio del modelador para su selección. Pero esto conlleva unos problemas, si K es muy pequeño entonces tendremos alta varianza pero poco sesgo, mientras que si K es muy alto no tendremos alta varianza pero si un alto sesgo.\n",
    "</p>\n",
    "<img src= 'KNN.png',width=400,heigth=1000, alt=\"\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors=5, metric= 'minkowski', p=2)\n",
    "KNN_model = KNN.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2\n",
    "Establezca el nivel predictivo de un KNN para determinar si un correo es spam o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
