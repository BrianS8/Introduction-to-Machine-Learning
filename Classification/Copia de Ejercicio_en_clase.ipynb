{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> Ejercicio en clase </center> </h1>\n",
    "<p> El siguiente ejercicio en clase busca abordar un problema de clasificación con el uso de las distintas técnicas vistas en clase, para ello presentamos a continuación desde la exploración de la información, balanceo de los datos, hasta las distintas aproximaciones y comparaciones entre los modelos. </p>\n",
    "<p> Cada sección posee una serie de ejercicios que usted tendrá que solucionar, por el momento no se preocupe, que lo realizaremos como equipo </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "En este ejercicio buscamos detectar si una operación es un fraude o no, para ello utilizaremos distintas técnicas que nos permitan aproximarnos a la respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de datos\n",
    "En este ejercicio seleccionaremos una base de datos denominada como ```credit```, la cual contienen las transacciones realizadas en dos días con tarjetas de crédito en septiembre de 2013 en Europa. En estos datos se identifican 492 fraudes de 284,807 transacciones. El conjunto de datos está altamente desequilibrado, pues la clase positiva (fraudes) representa el 0.172% de todas las transacciones.\n",
    "\n",
    "Por temas de confidencialidad esta base de datos contiene información anonimizada con PCA,  (V1,...,V28), exceptuando las características  ```Tiempo``` y ```Cantidad```. La variable ```Tiempo``` contiene los segundos transcurridos entre cada transacción, mientras que la variable ```Cantidad``` es la Cantidad de la transacción, mientras que la variable ```Clase``` es la variable de respuesta y toma el valor 1 en caso de fraude y 0 en caso contrario.\n",
    "\n",
    "Esta base de datos la puede encontrar en el siguiente [link](https://www.kaggle.com/mlg-ulb/creditcardfraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting warnings\n",
      "\u001b[31m  ERROR: Could not find a version that satisfies the requirement warnings (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for warnings\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "# Data manipulation\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# Models\n",
    "import statsmodels.api as sm2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Metrics\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "from sklearn.metrics import confusion_matrix, precision_score, accuracy_score\n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score, roc_curve "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración de la base de datos\n",
    "Comencemos con una exploración básica de la información de créditos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('credit.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.759072e-12</td>\n",
       "      <td>-8.251146e-13</td>\n",
       "      <td>-9.655448e-13</td>\n",
       "      <td>8.321385e-13</td>\n",
       "      <td>1.649983e-13</td>\n",
       "      <td>4.248434e-13</td>\n",
       "      <td>-3.054696e-13</td>\n",
       "      <td>8.777981e-14</td>\n",
       "      <td>-1.179757e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.405785e-13</td>\n",
       "      <td>-5.723165e-13</td>\n",
       "      <td>-9.725860e-13</td>\n",
       "      <td>1.464148e-12</td>\n",
       "      <td>-6.987110e-13</td>\n",
       "      <td>-5.617884e-13</td>\n",
       "      <td>3.332082e-12</td>\n",
       "      <td>-3.518875e-12</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.759072e-12 -8.251146e-13 -9.655448e-13  8.321385e-13   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   1.649983e-13  4.248434e-13 -3.054696e-13  8.777981e-14 -1.179757e-12   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ... -3.405785e-13 -5.723165e-13 -9.725860e-13  1.464148e-12   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean  -6.987110e-13 -5.617884e-13  3.332082e-12 -3.518875e-12      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.Class.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "Realice una tabla en la que calcule el porcentaje de clases 0 y 1 que tenemos en la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escriba aquí su código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribución de Fraude')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = [\"#0101DF\", \"#DF0101\"]\n",
    "sns.countplot('Class', data=dataset, palette=colors)\n",
    "plt.title('Distribución de Fraude', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es claro que nuestra base se encuentra desbalanceada, es decir, tenemos muy poca información para identificar patrones de fraude en las transacciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalar / Estandarizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.783274</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.269825</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.983721</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.418291</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.670579</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   scaled_amount        V1        V2        V3        V4        V5        V6  \\\n",
       "0       1.783274 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1      -0.269825  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2       4.983721 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3       1.418291 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4       0.670579 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "\n",
       "         V7        V8        V9  ...       V20       V21       V22       V23  \\\n",
       "0  0.239599  0.098698  0.363787  ...  0.251412 -0.018307  0.277838 -0.110474   \n",
       "1 -0.078803  0.085102 -0.255425  ... -0.069083 -0.225775 -0.638672  0.101288   \n",
       "2  0.791461  0.247676 -1.514654  ...  0.524980  0.247998  0.771679  0.909412   \n",
       "3  0.237609  0.377436 -1.387024  ... -0.208038 -0.108300  0.005274 -0.190321   \n",
       "4  0.592941 -0.270533  0.817739  ...  0.408542 -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Class  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_scaler = StandardScaler()\n",
    "rob_scaler = RobustScaler()\n",
    "\n",
    "dataset['scaled_amount'] = rob_scaler.fit_transform(dataset['Amount'].values.reshape(-1,1))\n",
    "dataset['scaled_time'] = rob_scaler.fit_transform(dataset['Time'].values.reshape(-1,1))\n",
    "\n",
    "dataset.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "scaled_amount = dataset['scaled_amount']\n",
    "scaled_time = dataset['scaled_time']\n",
    "\n",
    "dataset.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "dataset.insert(0, 'scaled_amount', scaled_amount)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanceo de la base de datos\n",
    "Para este caso, existen múltiples soluciones, entre ellas:\n",
    "1. No tener en cuenta las métricas de precisión y puntajes F1 a la hora de validar los modelos\n",
    "2. Trabajar con algoritmos adecuados - Random Forest\n",
    "3. Trabajar con técnicas de remuestreo que aumente o disminuyan la información\n",
    "4. Generación de muestras sintéticas\n",
    "\n",
    "En este ejercicio se mostraran las distintas técnicas, pero solo se trabajara con una de ellas, como ejercicio usted deberá seleccionar la muestra que mejor se adecue a lo que está buscando\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducción de la muestra de la clase dominante con remuestreo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    492\n",
       "0    492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset.sample(frac=1)\n",
    "\n",
    "df_fraude = df.loc[df['Class'] == 1]\n",
    "df_n_fraude = df.loc[df['Class'] == 0][:492]\n",
    "\n",
    "df_tamaño = pd.concat([df_fraude, df_n_fraude])\n",
    "df = df_tamaño.sample(frac=1, random_state=42)\n",
    "\n",
    "df.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    492\n",
       "0    492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fraude = dataset.loc[dataset['Class'] == 1]\n",
    "df_n_fraude = dataset.loc[dataset['Class'] == 0]\n",
    "\n",
    "df_n_fraude = resample(df_n_fraude,\n",
    "                       replace = False,\n",
    "                       n_samples = len(df_fraude),\n",
    "                       random_state = 27)\n",
    "\n",
    "df2 = pd.concat([df_n_fraude, df_fraude])\n",
    "\n",
    "df2.Class.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aumento de la muestra de la clase no dominante con remuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    213245\n",
       "0    213245\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = dataset.Class\n",
    "X = dataset.drop('Class', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n",
    "X = pd.concat([X_train, y_train], axis=1)\n",
    "df_n_fraude = X[X.Class==0]\n",
    "df_fraude = X[X.Class==1]\n",
    "\n",
    "df3 = resample(df_fraude,\n",
    "               replace=True,\n",
    "               n_samples=len(df_n_fraude),\n",
    "               random_state=27)\n",
    "\n",
    "df3 = pd.concat([df_n_fraude, df3])\n",
    "\n",
    "df3.Class.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanceo con muestra sintentica\n",
    "Esta técnica busca un submuestre de la información con los vecinos más cercanos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     0, 213245],\n",
       "       [     1, 213245]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = dataset.Class\n",
    "X = dataset.drop('Class', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n",
    "sm = SMOTE(random_state=27, ratio=1.0)\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "np.array(np.unique(y_train, return_counts=True)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con distintos modelos\n",
    "\n",
    "Al tener balanceada nuestra información balanceada, procederemos a evaluar los distintos modelos y seleccionar aquel que mejor resultado nos arroje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression(solver='lbfgs')\n",
    "logit_model = logit.fit(X_train,y_train)\n",
    "logit_pred = logit_model.predict(X_test)\n",
    "logit_roc_auc = roc_auc_score(y_test, logit_pred)\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.23it/s, best loss: -0.9125067415744194]\n",
      "best:\n",
      "{'alpha': 0.13054847911498313}\n"
     ]
    }
   ],
   "source": [
    "def hyperopt_train_test(params):\n",
    "    clf = BernoulliNB(**params)\n",
    "    return cross_val_score(clf, X_train, y_train).mean()\n",
    "space4knn = {\n",
    "    'alpha': hp.uniform('alpha', 0.0, 2.0)\n",
    "}\n",
    "\n",
    "def f(params):\n",
    "    acc = hyperopt_train_test(params)\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "trials = Trials()\n",
    "best = fmin(f, space4knn, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "print('best:')\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaiveBayes = BernoulliNB(alpha=0.09827861224760026)\n",
    "NB_model = NaiveBayes.fit(X_train,y_train)\n",
    "NB_pred = NB_model.predict(X_test)\n",
    "NB_roc_auc = roc_auc_score(y_test, NB_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
     ]
    }
   ],
   "source": [
    "def hyperopt_train_test(params):\n",
    "    clf = KNeighborsClassifier(**params)\n",
    "    return cross_val_score(clf, X_train, y_train).mean()\n",
    "space4knn = {\n",
    "    'n_neighbors': hp.choice('n_neighbors', range(2,10))\n",
    "}\n",
    "def f(params):\n",
    "    acc = hyperopt_train_test(params)\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "trials = Trials()\n",
    "best = fmin(f, space4knn, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "print('best:')\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2f73c24833d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mKNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mKNN_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mKNN_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mKNN_roc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKNN_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/neighbors/classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    452\u001b[0m                 delayed_query(\n\u001b[1;32m    453\u001b[0m                     self._tree, X[s], n_neighbors, return_distance)\n\u001b[0;32m--> 454\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m             )\n\u001b[1;32m    456\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36m_tree_query_parallel_helper\u001b[0;34m(tree, data, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0munder\u001b[0m \u001b[0mPyPy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \"\"\"\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors=2)\n",
    "KNN_model = KNN.fit(X_train,y_train)\n",
    "KNN_pred = KNN_model.predict(X_test)\n",
    "KNN_roc_auc = roc_auc_score(y_test, KNN_pred)\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:39<00:00,  4.00s/it, best loss: -0.9953246273177037]\n",
      "best:\n",
      "{'criterion': 0, 'max_depth': 16, 'max_features': 3}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "def hyperopt_train_test(params):\n",
    "    clf = DecisionTreeClassifier(**params)\n",
    "    return cross_val_score(clf, X_train, y_train).mean()\n",
    "space4dt = {\n",
    "    'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "    'max_features': hp.choice('max_features', range(1,5)),\n",
    "    'criterion': hp.choice('criterion', [\"gini\", \"entropy\"])\n",
    "}\n",
    "def f(params):\n",
    "    acc = hyperopt_train_test(params)\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(f, space4dt, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "print('best:')\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = DecisionTreeClassifier(criterion = \"gini\", max_depth = 16, max_features = 3)\n",
    "DT.fit(X_train,y_train)\n",
    "DT_pred = DT.predict(X_test)\n",
    "DT_roc_auc = roc_auc_score(y_test, DT_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:13<00:00, 25.38s/it, best loss: -0.9996576705123811]\n",
      "best:\n",
      "{'criterion': 0, 'max_depth': 16, 'max_features': 2, 'n_estimators': 12}\n"
     ]
    }
   ],
   "source": [
    "def hyperopt_train_test(params):\n",
    "    clf = RandomForestClassifier(**params)\n",
    "    return cross_val_score(clf, X_train, y_train).mean()\n",
    "space4rf = {\n",
    "    'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "    'max_features': hp.choice('max_features', range(1,5)),\n",
    "    'n_estimators': hp.choice('n_estimators', range(1,20)),\n",
    "    'criterion': hp.choice('criterion', [\"gini\", \"entropy\"])\n",
    "}\n",
    "best = 0\n",
    "def f(params):\n",
    "    acc = hyperopt_train_test(params)\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "trials = Trials()\n",
    "best = fmin(f, space4rf, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "print('best:')\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "RF = RandomForestClassifier(criterion=\"gini\", max_depth=16, max_features=2, \n",
    "                            n_estimators=12)\n",
    "RF.fit(X_train,y_train)\n",
    "RF_pred = RF.predict(X_test)\n",
    "RF_roc_auc = roc_auc_score(y_test, RF_pred)\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [07:32<00:00, 150.78s/it, best loss: -0.999758493630358]\n",
      "best:\n",
      "{'learning_rate': 0.18726176049598348, 'max_depth': 17, 'max_features': 1, 'n_estimators': 11}\n"
     ]
    }
   ],
   "source": [
    "def hyperopt_train_test(params):\n",
    "    clf = GradientBoostingClassifier(**params)\n",
    "    return cross_val_score(clf, X_train, y_train).mean()\n",
    "space4rf = {\n",
    "    'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "    'max_features': hp.choice('max_features', range(1,5)),\n",
    "    'n_estimators': hp.choice('n_estimators', range(1,20)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(1))}\n",
    "best = 0\n",
    "def f(params):\n",
    "    acc = hyperopt_train_test(params)\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "trials = Trials()\n",
    "best = fmin(f, space4rf, algo=tpe.suggest, max_evals=3, trials=trials)\n",
    "print('best:')\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB = GradientBoostingClassifier(n_estimators=20, learning_rate=0.25, max_features=2, max_depth=2, random_state=0)\n",
    "GB.fit(X_train, y_train)\n",
    "GB_pred = GB.predict(X_test)\n",
    "GB_roc_auc = roc_auc_score(y_test, GB_pred)\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación entre los distintos modelos\n",
    "En este caso seleccionamos el criterio de la curva de ROC para seleccionar cual es el mejor modelo.\n",
    "\n",
    "El área bajo la curva (AUC) y la curva ROC son métricas de evaluación para verificar el rendimiento de un modelo de clasificación\n",
    "\n",
    "La curva ROC nos dice que tan bueno puede distiguir un modelo entre 2 clases distintas\n",
    "Mientras que AUC, mide la probabilidad de que el modelo clasifique un ejemplo positivo aleatorio más alto que un ejemplo negativo aleatorio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Regresión Logit: ',logit_roc_auc)\n",
    "print('Naïve Bayes: ',NB_roc_auc)\n",
    "print('K Near Neighbor: ',KNN_roc_auc)\n",
    "print('Árbol de decisión: ',DT_roc_auc)\n",
    "print('Random Forest: ',RF_roc_auc)\n",
    "print('Gradient Boosting: ',GB_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_fpr, logit_tpr, thresholds = roc_curve(y_test, logit.predict_log_proba(X_test)[:,1])\n",
    "NB_fpr, NB_tpr, thresholds = roc_curve(y_test, NB_model.predict_proba(X_test)[:, 1])\n",
    "KNN_fpr, KNN_tpr, thresholds = roc_curve(y_test, KNN_model.predict_proba(X_test)[:, 1])\n",
    "DT_fpr, DT_tpr, thresholds = roc_curve(y_test, DT.predict_proba(X_test)[:, 1])\n",
    "RF_fpr, RF_tpr, thresholds = roc_curve(y_test, RF.predict_proba(X_test)[:, 1])\n",
    "GB_fpr, GB_tpr, thresholds = roc_curve(y_test, GB.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curva ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(logit_fpr, logit_tpr, label='Regresión Logística')\n",
    "plt.plot(NB_fpr, NB_tpr, label='NB')\n",
    "plt.plot(KNN_fpr, KNN_tpr, label='KNN')\n",
    "plt.plot(DT_fpr, DT_tpr, label='DT')\n",
    "plt.plot(RF_fpr, RF_tpr, label='RF')\n",
    "plt.plot(GB_fpr, GB_tpr, label='GB')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlabel('Tasa de Falsos Positivos')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "plt.title('Característica de funcionamiento del receptor (Curva ROC)')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "\n",
    "Con la base de emails, verifique que se encuentra balanceada, sino balancee la base con cualquier técnica y muestre cuál es la mejor técnica para predecir si un correo es spam o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.006350024818026649,\n",
       " 0.006414734386247674,\n",
       " 0.006480103373658278,\n",
       " 0.006546138500032374,\n",
       " 0.006612846553621324,\n",
       " 0.0066802343918518055,\n",
       " 0.006748308942030692,\n",
       " 0.006817077202057208,\n",
       " 0.006886546241142249,\n",
       " 0.006956723200535129,\n",
       " 0.007027615294257632,\n",
       " 0.007099229809845653,\n",
       " 0.007171574109098275,\n",
       " 0.007244655628834609,\n",
       " 0.007318481881658223,\n",
       " 0.007393060456729473,\n",
       " 0.007468399020545594,\n",
       " 0.007544505317728856,\n",
       " 0.007621387171822637,\n",
       " 0.007699052486095717,\n",
       " 0.0077775092443546695,\n",
       " 0.007856765511764592,\n",
       " 0.007936829435678219,\n",
       " 0.008017709246473386,\n",
       " 0.00809941325839916,\n",
       " 0.008181949870430461,\n",
       " 0.008265327567131522,\n",
       " 0.008349554919528008,\n",
       " 0.008434640585988173,\n",
       " 0.00852059331311285,\n",
       " 0.008607421936634645,\n",
       " 0.00869513538232616,\n",
       " 0.008783742666917614,\n",
       " 0.008873252899023668,\n",
       " 0.008963675280079844,\n",
       " 0.009055019105288331,\n",
       " 0.00914729376457359,\n",
       " 0.009240508743547532,\n",
       " 0.009334673624484703,\n",
       " 0.009429798087307237,\n",
       " 0.009525891910579998,\n",
       " 0.009622964972515738,\n",
       " 0.009721027251990574,\n",
       " 0.009820088829569827,\n",
       " 0.009920159888544205,\n",
       " 0.010021250715976706,\n",
       " 0.010123371703760016,\n",
       " 0.010226533349684864,\n",
       " 0.010330746258519069,\n",
       " 0.010436021143097786,\n",
       " 0.01054236882542466,\n",
       " 0.010649800237784404,\n",
       " 0.010758326423866506,\n",
       " 0.010867958539900595,\n",
       " 0.010978707855803183,\n",
       " 0.011090585756336266,\n",
       " 0.011203603742277576,\n",
       " 0.011317773431602906,\n",
       " 0.011433106560680344,\n",
       " 0.011549614985476775,\n",
       " 0.011667310682776657,\n",
       " 0.0117862057514132,\n",
       " 0.01190631241351206,\n",
       " 0.012027643015747777,\n",
       " 0.012150210030613008,\n",
       " 0.0122740260577006,\n",
       " 0.01239910382499887,\n",
       " 0.01252545619019996,\n",
       " 0.01265309614202161,\n",
       " 0.012782036801542324,\n",
       " 0.012912291423550257,\n",
       " 0.013043873397905682,\n",
       " 0.013176796250917536,\n",
       " 0.013311073646733798,\n",
       " 0.013446719388746196,\n",
       " 0.013583747421009134,\n",
       " 0.013722171829673111,\n",
       " 0.013862006844432747,\n",
       " 0.01400326683998956,\n",
       " 0.014145966337529644,\n",
       " 0.014290120006216427,\n",
       " 0.014435742664698603,\n",
       " 0.01458284928263348,\n",
       " 0.0147314549822258,\n",
       " 0.014881575039782283,\n",
       " 0.015033224887281982,\n",
       " 0.015186420113962661,\n",
       " 0.015341176467923337,\n",
       " 0.015497509857743107,\n",
       " 0.015655436354116592,\n",
       " 0.015814972191505863,\n",
       " 0.015976133769809407,\n",
       " 0.016138937656047906,\n",
       " 0.01630340058606735,\n",
       " 0.016469539466259415,\n",
       " 0.016637371375299404,\n",
       " 0.016806913565901895,\n",
       " 0.016978183466594268,\n",
       " 0.01715119868350834,\n",
       " 0.0173259770021902,\n",
       " 0.017502536389428544,\n",
       " 0.017680894995101618,\n",
       " 0.01786107115404296,\n",
       " 0.018043083387926206,\n",
       " 0.01822695040716904,\n",
       " 0.018412691112856615,\n",
       " 0.018600324598684524,\n",
       " 0.018789870152921537,\n",
       " 0.018981347260392516,\n",
       " 0.019174775604481244,\n",
       " 0.019370175069153985,\n",
       " 0.01956756574100336,\n",
       " 0.019766967911313355,\n",
       " 0.019968402078145056,\n",
       " 0.020171888948443954,\n",
       " 0.020377449440168435,\n",
       " 0.020585104684440185,\n",
       " 0.020794876027716385,\n",
       " 0.021006785033984076,\n",
       " 0.021220853486976887,\n",
       " 0.021437103392414357,\n",
       " 0.021655556980264053,\n",
       " 0.021876236707026774,\n",
       " 0.022099165258045016,\n",
       " 0.02232436554983498,\n",
       " 0.022551860732442322,\n",
       " 0.0227816741918219,\n",
       " 0.02301382955224189,\n",
       " 0.023248350678712146,\n",
       " 0.023485261679437645,\n",
       " 0.023724586908296562,\n",
       " 0.023966350967343994,\n",
       " 0.024210578709340784,\n",
       " 0.024457295240308524,\n",
       " 0.024706525922110208,\n",
       " 0.02495829637505752,\n",
       " 0.02521263248054445,\n",
       " 0.025469560383707884,\n",
       " 0.02572910649611523,\n",
       " 0.025991297498479483,\n",
       " 0.02625616034340194,\n",
       " 0.02652372225814284,\n",
       " 0.026794010747420285,\n",
       " 0.027067053596237654,\n",
       " 0.02734287887273983,\n",
       " 0.02762151493109849,\n",
       " 0.02790299041442698,\n",
       " 0.028187334257724572,\n",
       " 0.02847457569085111,\n",
       " 0.028764744241531564,\n",
       " 0.029057869738391615,\n",
       " 0.02935398231402376,\n",
       " 0.02965311240808509,\n",
       " 0.029955290770426184,\n",
       " 0.0302605484642523,\n",
       " 0.03056891686931652,\n",
       " 0.0308804276851455,\n",
       " 0.031195112934298157,\n",
       " 0.031513004965657454,\n",
       " 0.031834136457755825,\n",
       " 0.032158540422134445,\n",
       " 0.03248625020673672,\n",
       " 0.03281729949933638,\n",
       " 0.033151722331000534,\n",
       " 0.033489553079587876,\n",
       " 0.033830826473282845,\n",
       " 0.034175577594165334,\n",
       " 0.034523841881817334,\n",
       " 0.03487565513696574,\n",
       " 0.03523105352516286,\n",
       " 0.0355900735805039,\n",
       " 0.03595275220938281,\n",
       " 0.03631912669428591,\n",
       " 0.03668923469762475,\n",
       " 0.03706311426560742,\n",
       " 0.0374408038321498,\n",
       " 0.037822342222826405,\n",
       " 0.038207768658861566,\n",
       " 0.03859712276116127,\n",
       " 0.03899044455438611,\n",
       " 0.03938777447106573,\n",
       " 0.03978915335575514,\n",
       " 0.04019462246923352,\n",
       " 0.04060422349274564,\n",
       " 0.04101799853228668,\n",
       " 0.04143599012293049,\n",
       " 0.04185824123320233,\n",
       " 0.042284795269495606,\n",
       " 0.0427156960805343,\n",
       " 0.04315098796188014,\n",
       " 0.043590715660486466,\n",
       " 0.04403492437929772,\n",
       " 0.04448365978189656,\n",
       " 0.044936967997197666,\n",
       " 0.045394895624189864,\n",
       " 0.04585748973672635,\n",
       " 0.046324797888363724,\n",
       " 0.04679686811725043,\n",
       " 0.04727374895106494,\n",
       " 0.047755489412004276,\n",
       " 0.048242139021823394,\n",
       " 0.048733747806925874,\n",
       " 0.049230366303506544,\n",
       " 0.04973204556274648,\n",
       " 0.05023883715606082,\n",
       " 0.050750793180400475,\n",
       " 0.05126796626360718,\n",
       " 0.051790409569823906,\n",
       " 0.05231817680495961,\n",
       " 0.05285132222221047,\n",
       " 0.053389900627636606,\n",
       " 0.053933967385796366,\n",
       " 0.05448357842543733,\n",
       " 0.05503879024524591,\n",
       " 0.05559965991965516,\n",
       " 0.0561662451047119,\n",
       " 0.056738604044003646,\n",
       " 0.057316795574645876,\n",
       " 0.05790087913333038,\n",
       " 0.05849091476243516,\n",
       " 0.05908696311619671,\n",
       " 0.059689085466945055,\n",
       " 0.06029734371140243,\n",
       " 0.06091180037704613,\n",
       " 0.061532518628536155,\n",
       " 0.06215956227420838,\n",
       " 0.06279299577263393,\n",
       " 0.0634328842392454,\n",
       " 0.06407929345303036,\n",
       " 0.06473228986329357,\n",
       " 0.06539194059648763,\n",
       " 0.06605831346311344,\n",
       " 0.06673147696469096,\n",
       " 0.06741150030080102,\n",
       " 0.06809845337619888,\n",
       " 0.06879240680800021,\n",
       " 0.06949343193294043,\n",
       " 0.07020160081470787,\n",
       " 0.07091698625135189,\n",
       " 0.07163966178276614,\n",
       " 0.07236970169824847,\n",
       " 0.07310718104413756,\n",
       " 0.07385217563152759,\n",
       " 0.07460476204406137,\n",
       " 0.07536501764580303,\n",
       " 0.07613302058919076,\n",
       " 0.07690884982307077,\n",
       " 0.07769258510081306,\n",
       " 0.07848430698850971,\n",
       " 0.07928409687325709,\n",
       " 0.08009203697152217,\n",
       " 0.08090821033759413,\n",
       " 0.0817327008721222,\n",
       " 0.08256559333074043,\n",
       " 0.08340697333278033,\n",
       " 0.08425692737007238,\n",
       " 0.08511554281583716,\n",
       " 0.0859829079336671,\n",
       " 0.08685911188659975,\n",
       " 0.0877442447462836,\n",
       " 0.0886383975022371,\n",
       " 0.08954166207120229,\n",
       " 0.09045413130659352,\n",
       " 0.09137589900804263,\n",
       " 0.0923070599310413,\n",
       " 0.09324770979668165,\n",
       " 0.09419794530149624,\n",
       " 0.095157864127398,\n",
       " 0.09612756495172205,\n",
       " 0.0971071474573692,\n",
       " 0.09809671234305326,\n",
       " 0.09909636133365254,\n",
       " 0.100106197190667,\n",
       " 0.10112632372278181,\n",
       " 0.10215684579653869,\n",
       " 0.1031978693471159,\n",
       " 0.10424950138921814,\n",
       " 0.1053118500280774,\n",
       " 0.1063850244705659,\n",
       " 0.10746913503642232,\n",
       " 0.10856429316959236,\n",
       " 0.109670611449685,\n",
       " 0.11078820360354526,\n",
       " 0.11191718451694521,\n",
       " 0.11305767024639382,\n",
       " 0.11420977803106738,\n",
       " 0.11537362630486139,\n",
       " 0.11654933470856511,\n",
       " 0.11773702410216064,\n",
       " 0.1189368165772468,\n",
       " 0.12014883546959002,\n",
       " 0.12137320537180275,\n",
       " 0.12261005214615149,\n",
       " 0.12385950293749502,\n",
       " 0.12512168618635458,\n",
       " 0.12639673164211726,\n",
       " 0.127684770376374,\n",
       " 0.12898593479639323,\n",
       " 0.13030035865873227,\n",
       " 0.13162817708298705,\n",
       " 0.13296952656568203,\n",
       " 0.1343245449943018,\n",
       " 0.13569337166146553,\n",
       " 0.137076147279246,\n",
       " 0.13847301399363432,\n",
       " 0.1398841153991523,\n",
       " 0.14130959655361372,\n",
       " 0.14274960399303546,\n",
       " 0.1442042857467017,\n",
       " 0.14567379135238054,\n",
       " 0.14715827187169622,\n",
       " 0.14865787990565796,\n",
       " 0.15017276961034687,\n",
       " 0.15170309671276289,\n",
       " 0.15324901852683312,\n",
       " 0.15481069396958336,\n",
       " 0.1563882835774743,\n",
       " 0.15798194952290442,\n",
       " 0.15959185563088074,\n",
       " 0.16121816739585984,\n",
       " 0.16286105199876008,\n",
       " 0.1645206783241476,\n",
       " 0.1661972169775971,\n",
       " 0.16789084030322968,\n",
       " 0.16960172240142954,\n",
       " 0.17133003914674097,\n",
       " 0.17307596820594764,\n",
       " 0.1748396890563368,\n",
       " 0.17662138300414865,\n",
       " 0.17842123320321449,\n",
       " 0.18023942467378423,\n",
       " 0.18207614432154623,\n",
       " 0.1839315809568407,\n",
       " 0.18580592531406886,\n",
       " 0.1876993700713001,\n",
       " 0.18961210987007873,\n",
       " 0.1915443413354327,\n",
       " 0.19349626309608617,\n",
       " 0.19546807580487807,\n",
       " 0.19745998215938873,\n",
       " 0.19947218692277663,\n",
       " 0.2015048969448277,\n",
       " 0.20355832118321887,\n",
       " 0.20563267072499838,\n",
       " 0.20772815880828516,\n",
       " 0.2098450008441892,\n",
       " 0.21198341443895483,\n",
       " 0.2141436194163309,\n",
       " 0.2163258378401675,\n",
       " 0.21853029403724394,\n",
       " 0.22075721462032877,\n",
       " 0.22300682851147513,\n",
       " 0.22527936696555342,\n",
       " 0.22757506359402363,\n",
       " 0.2298941543889502,\n",
       " 0.23223687774726126,\n",
       " 0.23460347449525532,\n",
       " 0.2369941879133577,\n",
       " 0.2394092637611291,\n",
       " 0.24184895030252904,\n",
       " 0.24431349833143698,\n",
       " 0.24680316119743306,\n",
       " 0.24931819483184212,\n",
       " 0.2518588577740425,\n",
       " 0.25442541119804357,\n",
       " 0.2570181189393337,\n",
       " 0.25963724752200146,\n",
       " 0.26228306618613456,\n",
       " 0.2649558469154963,\n",
       " 0.26765586446548517,\n",
       " 0.27038339639137904,\n",
       " 0.27313872307686693,\n",
       " 0.2759221277628722,\n",
       " 0.27873389657666875,\n",
       " 0.28157431856129406,\n",
       " 0.28444368570526246,\n",
       " 0.28734229297258074,\n",
       " 0.29027043833306965,\n",
       " 0.29322842279299444,\n",
       " 0.29621655042600775,\n",
       " 0.2992351284044076,\n",
       " 0.3022844670307137,\n",
       " 0.3053648797695663,\n",
       " 0.308476683279949,\n",
       " 0.3116201974477413,\n",
       " 0.3147957454186009,\n",
       " 0.3180036536311837,\n",
       " 0.3212442518507,\n",
       " 0.32451787320281367,\n",
       " 0.327824854207887,\n",
       " 0.3311655348155738,\n",
       " 0.33454025843976576,\n",
       " 0.33794937199389413,\n",
       " 0.34139322592659205,\n",
       " 0.34487217425771943,\n",
       " 0.3483865746147557,\n",
       " 0.3519367882695627,\n",
       " 0.3555231801755228,\n",
       " 0.3591461190050551,\n",
       " 0.3628059771875142,\n",
       " 0.3665031309474747,\n",
       " 0.37023796034340667,\n",
       " 0.37401084930674444,\n",
       " 0.3778221856813538,\n",
       " 0.3816723612634017,\n",
       " 0.38556177184163115,\n",
       " 0.38949081723804857,\n",
       " 0.39345990134902364,\n",
       " 0.3974694321868092,\n",
       " 0.4015198219214836,\n",
       " 0.4056114869233214,\n",
       " 0.40974484780559445,\n",
       " 0.4139203294678107,\n",
       " 0.41813836113939223,\n",
       " 0.42239937642379927,\n",
       " 0.42670381334310364,\n",
       " 0.43105211438301616,\n",
       " 0.43544472653837335,\n",
       " 0.4398821013590871,\n",
       " 0.4443646949965633,\n",
       " 0.44889296825059277,\n",
       " 0.45346738661672065,\n",
       " 0.4580884203340978,\n",
       " 0.4627565444338208,\n",
       " 0.4674722387877638,\n",
       " 0.4722359881579074,\n",
       " 0.4770482822461724,\n",
       " 0.4819096157447591,\n",
       " 0.48682048838700065,\n",
       " 0.49178140499873435,\n",
       " 0.4967928755501967,\n",
       " 0.5018554152084469,\n",
       " 0.5069695443903249,\n",
       " 0.5121357888159492,\n",
       " 0.5173546795627592,\n",
       " 0.5226267531201089,\n",
       " 0.527952551444416,\n",
       " 0.5333326220148761,\n",
       " 0.5387675178897384,\n",
       " 0.544257797763163,\n",
       " 0.549804026022649,\n",
       " 0.5554067728070565,\n",
       " 0.5610666140652113,\n",
       " 0.5667841316151151,\n",
       " 0.572559913203751,\n",
       " 0.5783945525675064,\n",
       " 0.5842886494932037,\n",
       " 0.5902428098797612,\n",
       " 0.5962576458004732,\n",
       " 0.6023337755659351,\n",
       " 0.6084718237875987,\n",
       " 0.6146724214419856,\n",
       " 0.620936205935546,\n",
       " 0.6272638211701864,\n",
       " 0.6336559176094578,\n",
       " 0.6401131523454228,\n",
       " 0.6466361891662055,\n",
       " 0.6532256986242225,\n",
       " 0.65988235810512,\n",
       " 0.6666068518974009,\n",
       " 0.6733998712627743,\n",
       " 0.6802621145072102,\n",
       " 0.6871942870527283,\n",
       " 0.694197101509909,\n",
       " 0.7012712777511528,\n",
       " 0.708417542984677,\n",
       " 0.7156366318292752,\n",
       " 0.7229292863898297,\n",
       " 0.7302962563336034,\n",
       " 0.737738298967298,\n",
       " 0.7452561793149095,\n",
       " 0.7528506701963641,\n",
       " 0.760522552306969,\n",
       " 0.7682726142976605,\n",
       " 0.7761016528560774,\n",
       " 0.784010472788461,\n",
       " 0.7919998871023824,\n",
       " 0.8000707170903228,\n",
       " 0.8082237924140953,\n",
       " 0.8164599511901374,\n",
       " 0.8247800400756617,\n",
       " 0.8331849143556967,\n",
       " 0.8416754380310008,\n",
       " 0.8502524839068866,\n",
       " 0.8589169336829365,\n",
       " 0.8676696780436463,\n",
       " 0.8765116167499777,\n",
       " 0.8854436587318592,\n",
       " 0.8944667221816148,\n",
       " 0.903581734648359,\n",
       " 0.9127896331333392,\n",
       " 0.9220913641862647,\n",
       " 0.9314878840026023,\n",
       " 0.9409801585218782,\n",
       " 0.9505691635269681,\n",
       " 0.9602558847444074,\n",
       " 0.970041317945725,\n",
       " 0.9799264690497994,\n",
       " 0.9899123542262721,\n",
       " 1.0]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.logspace(np.log(0.01), np.log(1), num = 500, base=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
